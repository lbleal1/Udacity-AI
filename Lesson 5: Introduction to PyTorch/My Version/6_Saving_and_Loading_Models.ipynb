{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "6  Saving and Loading Models",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJkNgMhZA-7",
        "colab_type": "text"
      },
      "source": [
        "# Previously\n",
        "\n",
        "We learned how to do validation pass where we learned about accuracy which is one of the performance metrics, improve performance and do inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ROrCYTZ5Vj",
        "colab_type": "text"
      },
      "source": [
        "# In this notebook, \n",
        "\n",
        "we will learn how to:\n",
        "- save and load models in PyTorch \n",
        "\n",
        "**Why learn Saving and Loading Models?**\n",
        "- you'll often want to load previously trained models to use in making predictions or to continue training on new data.\n",
        "- it's impractical to train a network every time you need to use it\n",
        "\n",
        "Thus,\n",
        "we instead save trained networks then load them later to train more or use them for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG8UQ5kRgmdG",
        "colab_type": "text"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxJNL8g1goFv",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Loading Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXHxyjjSh917",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "8c2c14d5-10c4-4dcf-ce90-5daf9720f813"
      },
      "source": [
        "# folks the helper package always goes in the way as with the previous notebooks\n",
        "# so don't forget this one if you're running in colab\n",
        "\n",
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py\n",
        "\n",
        "# if this doesn't still get through by running, reset your runtime and\n",
        "# it will work\n",
        "\n",
        "# also don't forget to change your Hardware Accelerator above to GPU for faster processing\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lbleal1/deep-learning-v2-pytorch/master/intro-to-pytorch/fc_model.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-11 12:43:15--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2813 (2.7K) [text/plain]\n",
            "Saving to: ‘helper.py’\n",
            "\n",
            "\rhelper.py             0%[                    ]       0  --.-KB/s               \rhelper.py           100%[===================>]   2.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-11 12:43:15 (61.6 MB/s) - ‘helper.py’ saved [2813/2813]\n",
            "\n",
            "--2020-01-11 12:43:17--  https://raw.githubusercontent.com/lbleal1/deep-learning-v2-pytorch/master/intro-to-pytorch/fc_model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3543 (3.5K) [text/plain]\n",
            "Saving to: ‘fc_model.py’\n",
            "\n",
            "fc_model.py         100%[===================>]   3.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-11 12:43:17 (72.5 MB/s) - ‘fc_model.py’ saved [3543/3543]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTBPW4Dcgtpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import helper\n",
        "import fc_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt0w0FTAguFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "209ac623-7edc-4c86-de12-3f2d8d1b0d50"
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:02, 10152584.45it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 74691.67it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 3139157.75it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 26512.47it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsOEHWbygv2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "b581289d-78c1-4243-fcda-2ebf7a8fa71e"
      },
      "source": [
        "# looking at one of the images\n",
        "\n",
        "image, label = next(iter(trainloader))\n",
        "helper.imshow(image[0,:]);"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAHPCAYAAAA1eFErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBJPAAj9HwdblxgQkNsnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 231,
              "height": 231
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBCBqOx5gyt0",
        "colab_type": "text"
      },
      "source": [
        "# Training the Network\n",
        "\n",
        "To focus more in saving and loading model, we will be using `fc_model`\n",
        "\n",
        "`fc_model`\n",
        "- a file containing the model architecture and training code from the last part\n",
        "\n",
        "1. Import `fc_model`\n",
        "2. Create a fully-connected network with `fc_model.Network`\n",
        "3. Train the network using `fc_model.train`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4_WnU4whZrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the network, define the criterion and optimizer\n",
        "\n",
        "model = fc_model.Network(784, 10, [512, 256, 128])\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IdAUnSzhbgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "675ec9b7-e064-4214-e8c3-289c4e063516"
      },
      "source": [
        "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2..  Training Loss: 1.684..  Test Loss: 0.929..  Test Accuracy: 0.667\n",
            "Epoch: 1/2..  Training Loss: 1.047..  Test Loss: 0.726..  Test Accuracy: 0.717\n",
            "Epoch: 1/2..  Training Loss: 0.833..  Test Loss: 0.660..  Test Accuracy: 0.747\n",
            "Epoch: 1/2..  Training Loss: 0.784..  Test Loss: 0.640..  Test Accuracy: 0.761\n",
            "Epoch: 1/2..  Training Loss: 0.760..  Test Loss: 0.607..  Test Accuracy: 0.773\n",
            "Epoch: 1/2..  Training Loss: 0.727..  Test Loss: 0.603..  Test Accuracy: 0.780\n",
            "Epoch: 1/2..  Training Loss: 0.695..  Test Loss: 0.579..  Test Accuracy: 0.786\n",
            "Epoch: 1/2..  Training Loss: 0.675..  Test Loss: 0.565..  Test Accuracy: 0.781\n",
            "Epoch: 1/2..  Training Loss: 0.669..  Test Loss: 0.548..  Test Accuracy: 0.794\n",
            "Epoch: 1/2..  Training Loss: 0.643..  Test Loss: 0.550..  Test Accuracy: 0.785\n",
            "Epoch: 1/2..  Training Loss: 0.606..  Test Loss: 0.539..  Test Accuracy: 0.800\n",
            "Epoch: 1/2..  Training Loss: 0.657..  Test Loss: 0.540..  Test Accuracy: 0.798\n",
            "Epoch: 1/2..  Training Loss: 0.615..  Test Loss: 0.544..  Test Accuracy: 0.806\n",
            "Epoch: 1/2..  Training Loss: 0.615..  Test Loss: 0.515..  Test Accuracy: 0.811\n",
            "Epoch: 1/2..  Training Loss: 0.608..  Test Loss: 0.514..  Test Accuracy: 0.814\n",
            "Epoch: 1/2..  Training Loss: 0.564..  Test Loss: 0.530..  Test Accuracy: 0.796\n",
            "Epoch: 1/2..  Training Loss: 0.598..  Test Loss: 0.511..  Test Accuracy: 0.810\n",
            "Epoch: 1/2..  Training Loss: 0.574..  Test Loss: 0.523..  Test Accuracy: 0.806\n",
            "Epoch: 1/2..  Training Loss: 0.601..  Test Loss: 0.501..  Test Accuracy: 0.816\n",
            "Epoch: 1/2..  Training Loss: 0.615..  Test Loss: 0.508..  Test Accuracy: 0.815\n",
            "Epoch: 1/2..  Training Loss: 0.566..  Test Loss: 0.493..  Test Accuracy: 0.821\n",
            "Epoch: 1/2..  Training Loss: 0.564..  Test Loss: 0.484..  Test Accuracy: 0.813\n",
            "Epoch: 1/2..  Training Loss: 0.563..  Test Loss: 0.486..  Test Accuracy: 0.818\n",
            "Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.490..  Test Accuracy: 0.821\n",
            "Epoch: 2/2..  Training Loss: 0.558..  Test Loss: 0.483..  Test Accuracy: 0.821\n",
            "Epoch: 2/2..  Training Loss: 0.551..  Test Loss: 0.474..  Test Accuracy: 0.825\n",
            "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.473..  Test Accuracy: 0.824\n",
            "Epoch: 2/2..  Training Loss: 0.512..  Test Loss: 0.469..  Test Accuracy: 0.827\n",
            "Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.501..  Test Accuracy: 0.817\n",
            "Epoch: 2/2..  Training Loss: 0.541..  Test Loss: 0.476..  Test Accuracy: 0.825\n",
            "Epoch: 2/2..  Training Loss: 0.545..  Test Loss: 0.475..  Test Accuracy: 0.825\n",
            "Epoch: 2/2..  Training Loss: 0.565..  Test Loss: 0.471..  Test Accuracy: 0.831\n",
            "Epoch: 2/2..  Training Loss: 0.531..  Test Loss: 0.462..  Test Accuracy: 0.829\n",
            "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.465..  Test Accuracy: 0.830\n",
            "Epoch: 2/2..  Training Loss: 0.529..  Test Loss: 0.453..  Test Accuracy: 0.832\n",
            "Epoch: 2/2..  Training Loss: 0.556..  Test Loss: 0.461..  Test Accuracy: 0.838\n",
            "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.443..  Test Accuracy: 0.834\n",
            "Epoch: 2/2..  Training Loss: 0.504..  Test Loss: 0.445..  Test Accuracy: 0.837\n",
            "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.431..  Test Accuracy: 0.842\n",
            "Epoch: 2/2..  Training Loss: 0.543..  Test Loss: 0.452..  Test Accuracy: 0.833\n",
            "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.456..  Test Accuracy: 0.834\n",
            "Epoch: 2/2..  Training Loss: 0.486..  Test Loss: 0.451..  Test Accuracy: 0.833\n",
            "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.441..  Test Accuracy: 0.835\n",
            "Epoch: 2/2..  Training Loss: 0.504..  Test Loss: 0.461..  Test Accuracy: 0.832\n",
            "Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.444..  Test Accuracy: 0.837\n",
            "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.456..  Test Accuracy: 0.833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SadzA0phcPP",
        "colab_type": "text"
      },
      "source": [
        "# (2) Two Ways to Save and Load Models\n",
        "\n",
        "One of the key here is the model's `state_dict`\n",
        "\n",
        "`state_dict`\n",
        "- contains the weight and bias matrices for each of our layers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5BVySWgiMii",
        "colab_type": "text"
      },
      "source": [
        "## 1. Saving and Loading only the Model Parameters\n",
        "- here we only load and save `state_dict` \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMamYuFOjAD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "66c1e6c9-f8eb-47ee-f363-10454967e508"
      },
      "source": [
        "# checking the state dict keys\n",
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jcw4GPVjGny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Saving the model parameters to a file 'checkpoint.pth'\n",
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhsdACcjU8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "94e78906-5c5c-4bdb-9e5c-c57fe654c621"
      },
      "source": [
        "# 2. Load it back\n",
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncaoWkugjaqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a399e224-0688-43b7-fd31-1626925ce8db"
      },
      "source": [
        "# 3. Then load it into a network\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g76xsybijhAh",
        "colab_type": "text"
      },
      "source": [
        "**Problem:**\n",
        "- Since we're loading just the parameters, \n",
        "\n",
        "if the model architecture we're loading it into matches the checkpoint architecture:\n",
        "  - this will only work \n",
        "  \n",
        "else if different architecture\n",
        "  - this fails\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aloeNpgj_mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "915f5653-70b6-4ba5-ac90-3f06de3cd6f9"
      },
      "source": [
        "# Try this to see the problem\n",
        "model = fc_model.Network(784, 10, [400, 200, 100])\n",
        "# This will throw an error because the tensor sizes are wrong!\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cc11e1013989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnigYXg2kCFW",
        "colab_type": "text"
      },
      "source": [
        "## 2. Saving and Loading the Network Architecture and Parameters\n",
        "\n",
        "This means we're rebuilding the model exactly as it was when trained.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0bCErgWka-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Build a dictionary with all the information we need\n",
        "# to completely rebuild the model\n",
        "checkpoint = {'input_size': 784,\n",
        "              'output_size': 10,\n",
        "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "# 2. We save both the network architecture and its parameters\n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezRNLmT9krjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Loading both the network architecture and its parameter\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = fc_model.Network(checkpoint['input_size'],\n",
        "                             checkpoint['output_size'],\n",
        "                             checkpoint['hidden_layers'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK8x7u6mk1L_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "a7798b34-863a-4f18-de12-592a86132be0"
      },
      "source": [
        "# 4. Loading the saved network architecture and parameters to the current model\n",
        "\n",
        "model = load_checkpoint('checkpoint.pth')\n",
        "print(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3kYUNAFk7_y",
        "colab_type": "text"
      },
      "source": [
        "# Next Up\n",
        "\n",
        "This actually finishes our basic workflow from downloading and loading data to saving and loading models just as we just finished.\n",
        "\n",
        "The next question is, what if I do not download my dataset and want to use my own pictures? This will change our very first step and we'll learn how to load image data and the other details we haven't discussed before. "
      ]
    }
  ]
}