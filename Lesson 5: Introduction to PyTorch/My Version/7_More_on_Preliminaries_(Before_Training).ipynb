{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "7  - More on Preliminaries (Before Training)",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hbpU_na0Ted",
        "colab_type": "text"
      },
      "source": [
        "# Previously\n",
        "\n",
        "We finished saving and loading our models but we just download our dataset. In real life, most probably, that won't be the case. We want to process our own dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr-fqi4S1Wmu",
        "colab_type": "text"
      },
      "source": [
        "# In this notebook, \n",
        "\n",
        "We'll dig in more to our Preliminary Step. We'll\n",
        "- load our own dataset from a certain storage\n",
        "- do Transforms to our Image Data\n",
        "- get a detailed view of loading the data to be used in our model\n",
        "- learn about Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYQ7j5RH2fiF",
        "colab_type": "text"
      },
      "source": [
        "# Sample Dataset\n",
        "\n",
        "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
        "\n",
        "![Dog and Cat](https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/dog_cat.png?raw=true) \n",
        "\n",
        "**For raw datasets, you can expect:**\n",
        "- you need to resize it for training (depending on the neural network)\n",
        "- if it's few, you might wanna augment it (we'll discuss Data Augmentation later)\n",
        "\n",
        "*This is different from the dataset we had before because those are already prepared for training. So beware of the needed preparation depending on the context of the data and your neural network.\n",
        "\n",
        "***Remark:***\n",
        "- These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrthep2n6rcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7db87278-b520-49bf-e625-2b697a09f504"
      },
      "source": [
        "# folks the helper package always goes in the way as with the previous notebooks\n",
        "# so don't forget this one if you're running in colab\n",
        "\n",
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py\n",
        "\n",
        "# if this doesn't still get through by running, reset your runtime and\n",
        "# it will work\n",
        "\n",
        "# also don't forget to change your Hardware Accelerator above to GPU for faster processing"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-11 15:34:08--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2813 (2.7K) [text/plain]\n",
            "Saving to: ‘helper.py’\n",
            "\n",
            "\rhelper.py             0%[                    ]       0  --.-KB/s               \rhelper.py           100%[===================>]   2.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-11 15:34:09 (92.0 MB/s) - ‘helper.py’ saved [2813/2813]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-rrTEqEpQe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0xdH047fIB",
        "colab_type": "text"
      },
      "source": [
        "# Basics: Loading Image Data, Transform, Loading Data for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOf5eCNL7pha",
        "colab_type": "text"
      },
      "source": [
        "##Conceptual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUm5WNn97tiM",
        "colab_type": "text"
      },
      "source": [
        "### Loading Image Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1JSV063pQfG",
        "colab_type": "text"
      },
      "source": [
        "`datasets.ImageFolder`\n",
        "- used to load image data (see [documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder))\n",
        "-  expects the files and directories to be constructed like so:\n",
        "\n",
        "```\n",
        "root/dog/xxx.png\n",
        "root/dog/xxy.png\n",
        "root/dog/xxz.png\n",
        "\n",
        "root/cat/123.png\n",
        "root/cat/nsdf3.png\n",
        "root/cat/asd932_.png\n",
        "```\n",
        "\n",
        "where each class has it's own directory (`cat` and `dog`) for the images.\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMRDEXW38Yk1",
        "colab_type": "text"
      },
      "source": [
        "### Transforms\n",
        "\n",
        "`transform` \n",
        "- a sequence of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder\n",
        "  *The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. \n",
        "- done after loading in the data with `ImageFolder`\n",
        "- this is used to let the images conform to the descriptions we want it to have such as:\n",
        "  - we want it to have a uniform size, so we do\n",
        "    - resize them with `transforms.Resize()`\n",
        "    - or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc.\n",
        "  - we want it to convert to tensors\n",
        "    - so we'll use `transforms.ToTensor()`\n",
        "  - we want to combine these transforms into pipeline with `transforms.Compose()`\n",
        "    - which accepts a list of transforms and runs them in sequence.\n",
        "- Example:\n",
        "    ```python\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "    ```\n",
        "\n",
        "There are plenty of transforms available, I'll cover more in a bit and you can read through the [documentation](http://pytorch.org/docs/master/torchvision/transforms.html). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rDmQ7-M9_qv",
        "colab_type": "text"
      },
      "source": [
        "### Loading your Data for Training with `DataLoader`\n",
        "\n",
        "**[`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader)**\n",
        "- takes a dataset (such as you would get from `ImageFolder`)\n",
        "- returns batches of images and the corresponding labels\n",
        "- *You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
        "\n",
        "```python\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "```\n",
        "\n",
        "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
        "\n",
        "**recall:** We have an option to get per batch manually or go through all the batches\n",
        "```python\n",
        "# Get one batch\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# Looping through it, get a batch on each loop \n",
        "for images, labels in dataloader:\n",
        "    pass\n",
        "\n",
        "```\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9R4jv68AzO",
        "colab_type": "text"
      },
      "source": [
        "## Coding the Basic Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRH8rNJJQ5rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICBmt7knRd4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/Udacity-AI Notes/Notebooks/L5 Notebooks/assets/dogs-vs-cats/train'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7HyUSvYpQfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this to test your data loader\n",
        "images, labels = next(iter(dataloader))\n",
        "helper.imshow(images[0], normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrsuRMjBpQfY",
        "colab_type": "text"
      },
      "source": [
        "If you loaded the data correctly, you should see something like this (your image will be different):\n",
        "\n",
        "![Cat](https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/cat_cropped.png?raw=true) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RY6Uc6XpQfa",
        "colab_type": "text"
      },
      "source": [
        "# Data Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjunRrSmSLxJ",
        "colab_type": "text"
      },
      "source": [
        "## Conceptual\n",
        "\n",
        "\n",
        "**Data Augmentation**\n",
        "- a common strategy for training neural networks is to introduce randomness in the input data itself\n",
        "- for example, you can randomly rotate, mirror, scale, and/or crop your images during training. \n",
        "- this will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n",
        "\n",
        "To randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n",
        "\n",
        "```python\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
        "                                                            [0.5, 0.5, 0.5])])\n",
        "```\n",
        "\n",
        "`transforms.Normalize`\n",
        "- you pass in a list of means and list of standard deviations, then the color channels are normalized like so\n",
        "- Normalizing helps keep the network weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n",
        "\n",
        "```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n",
        "\n",
        "  - Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. \n",
        "\n",
        "***Remark: Some Guidelines for Transforms***\n",
        "\n",
        "- You can find a list of all [the available transforms here](http://pytorch.org/docs/0.3.0/torchvision/transforms.html). \n",
        "- When you're testing however, you'll want to use images that aren't altered other than normalizing. \n",
        "- So, for validation/test images, you'll typically just resize and crop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH94vswxSyuO",
        "colab_type": "text"
      },
      "source": [
        "## Coding Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F001wE0pQfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/Udacity-AI Notes/Notebooks/L5 Notebooks/assets/dogs-vs-cats'\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor()]) \n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he7yqymJpQfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change this to the trainloader or testloader \n",
        "data_iter = iter(testloader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    helper.imshow(images[ii], ax=ax, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKyviweZpQft",
        "colab_type": "text"
      },
      "source": [
        "Your transformed images should look something like this.\n",
        "\n",
        "<center>Training examples:</center>\n",
        "\n",
        "![Train Examples](https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/train_examples.png?raw=true) \n",
        "\n",
        "<center>Testing examples:</center>\n",
        "\n",
        "![Test Examples](https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/test_examples.png?raw=true) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pum-Fh59pQfv",
        "colab_type": "text"
      },
      "source": [
        "# At this point,\n",
        "\n",
        "You should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won't get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you've seen 28x28 images which are tiny).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zl17vRmTOfE",
        "colab_type": "text"
      },
      "source": [
        "# Next Up!\n",
        "\n",
        "We'll learn how to use a pre-trained network to build a model that can actually solve this problem. This will be the final notebook and we will use all what we've learned from these past 7 notebooks to solve it. Til the next notebook!"
      ]
    }
  ]
}