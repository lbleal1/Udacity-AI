{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "1 Using PyTorch Tensors to build Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUYpJcJU0RWY",
        "colab_type": "text"
      },
      "source": [
        "# Using PyTorch Tensors to build Neural Networks\n",
        "\n",
        "In this notebook, \n",
        "- you'll get introduced to [PyTorch](http://pytorch.org/), \n",
        "  - a framework for building and training neural networks. \n",
        "  - PyTorch in a lot of ways behaves like the arrays you love from Numpy. \n",
        "     \n",
        "      *These Numpy arrays, after all, are just tensors.   \n",
        "  - PyTorch takes these tensors and makes it simple to \n",
        "      move them to GPUs for the faster processing needed \n",
        "      when training neural networks. \n",
        "  - It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. \n",
        "  \n",
        "  - *All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CXC0KjG0RWb",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer Neural Networks\n",
        "\n",
        "**Deep Learning**\n",
        "- based on artificial neural networks which have been around in some form since the late 1950s. \n",
        "\n",
        "  **Neurons**\n",
        "  - \"units\" or individual parts approxumating neurons by  \n",
        "     which networks are built. \n",
        "  - each unit has some number of **weighted inputs**. \n",
        "      \n",
        "      These weighted inputs are summed together (a linear \n",
        "      combination) then passed through an activation \n",
        "      function to get the unit's output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSkLxtnU_JCo",
        "colab_type": "text"
      },
      "source": [
        "### Conceptually\n",
        "\n",
        "**Big Picture**\n",
        "\n",
        "<img src=\"https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/simple_neuron.png?raw=1\" width=400px>\n",
        "\n",
        "\n",
        "\n",
        "**Mathematically**\n",
        "\n",
        "We can represent this into two ways. Though, I see these ways as kind of sequential. I learned this one from Andrew Ng's Machine Learning Course in Coursera--that we can represent summation(usually coded as for loops) as tensors. \n",
        "\n",
        "*From Summation*\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
        "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "*to Tensor Language*\n",
        "\n",
        "$$\n",
        "h = \\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots  x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_1 \\\\\n",
        "           w_2 \\\\\n",
        "           \\vdots \\\\\n",
        "           w_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= f(h) \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "***Remark: Why do this translation?***                    \n",
        "Well, it seems that in programming languages, tensor operations are faster than loops and it's kinda neat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMtI_dL0RWf",
        "colab_type": "text"
      },
      "source": [
        "### Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPt1hXUgAlGC",
        "colab_type": "text"
      },
      "source": [
        "#### Tensors\n",
        "\n",
        "***Remark:***                                                \n",
        "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*\n",
        "\n",
        "**Tensors**\n",
        "- a generalization of matrices. \n",
        "  \n",
        "  **Vector**\n",
        "   - a 1-dimensional tensor\n",
        "   \n",
        "  **Matrix**\n",
        "   - a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). \n",
        "   \n",
        "***Remark:***     \n",
        "The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
        "\n",
        "<img src=\"https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/tensor_examples.svg?raw=1\" width=600px>\n",
        "\n",
        "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1m4HEr60RWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, import PyTorch\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DI-me1FBYUm",
        "colab_type": "text"
      },
      "source": [
        "#### Building Single Layer Neural Networks\n",
        "\n",
        "So we need to code the following with the knowledge of tensors:\n",
        "1. The summation of the weights multiplies to features and the bias\n",
        "2. The activation function (our only function for now)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFO42BZ0RXC",
        "colab_type": "text"
      },
      "source": [
        "***Remark: PyTorch and Numpy***\n",
        "- PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. \n",
        "- In general, you'll use PyTorch tensors pretty much the same way you'd use Numpy arrays. \n",
        "- They come with some nice benefits though such as GPU acceleration which we'll get to later. \n",
        "  \n",
        "  *For now, use the generated data to calculate the output of this simple single layer network. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKs-eJUh0RW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Generate some data for the features, weights and bias\n",
        "torch.manual_seed(7) # Set the random seed so things are predictable\n",
        "\n",
        "features = torch.randn((1, 5)) # Features are 5 random normal variables\n",
        "weights = torch.randn_like(features) # True weights for our data, random normal variables again\n",
        "bias = torch.randn((1, 1)) # and a true bias term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Zb2Jha1-_M",
        "colab_type": "code",
        "outputId": "8650fe01-1acf-495c-aae0-2d1c88a3d21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(weights)\n",
        "print(bias)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n",
            "tensor([[0.3177]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBWKSVI80RWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(x):\n",
        "    \"\"\" Sigmoid activation function \n",
        "        x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1/(1+torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwgRUy_60RXG",
        "colab_type": "code",
        "outputId": "6ea36ceb-7f22-4dba-95d9-b1f646c28792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1. multiply the features and the weights then add the bias\n",
        "result = torch.matmul(weights, features.t()) + bias\n",
        "\n",
        "# 2. use activation function to get the output\n",
        "activation(result)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRdV93KADdlE",
        "colab_type": "text"
      },
      "source": [
        "##### Pytorch Implementation Details\n",
        "\n",
        "**Why use  `torch.matmul()`?**\n",
        "\n",
        "`matmul` stands for Matrix Multiplication. As we stated earlier, instead of summation, we can do tensor operations. \n",
        "\n",
        "Though this was touched previously, to emphasize and much more focused in this operation -- you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBT13k1X0RXP",
        "colab_type": "text"
      },
      "source": [
        "##### Shape Problems\n",
        "\n",
        "Noticed that we use `features.t()` above which means we transposed it so that the matrix multiplication would work but there's a more flexible way of doing this. \n",
        "\n",
        "\n",
        "\n",
        "**Note:** To see the shape of a tensor called `tensor`, use `tensor.shape`. If you're building neural networks, you'll be using this method often.\n",
        "\n",
        "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
        "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
        "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
        "\n",
        "Matt recommends using `.view()`, but any of the three methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`.\n",
        "\n",
        "So applying these, we modify our solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDX47DJC0RXS",
        "colab_type": "code",
        "outputId": "81e82aae-96db-4cbb-99af-2c48a0b9b3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1. multiply the features and the weights then add the bias\n",
        "result = torch.matmul(weights, features.view(5,1)) + bias\n",
        "\n",
        "# 2. use activation function to get the output\n",
        "activation(result)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GB3ocGr0RXd",
        "colab_type": "text"
      },
      "source": [
        "## Extending to Multi-Layer Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1GhkNqbJ7BU",
        "colab_type": "text"
      },
      "source": [
        "#### Conceptually\n",
        "\n",
        "***Remark:***                                             \n",
        "That's how you can calculate the output for a single neuron. The real power of this algorithm happens with multi-layer neural networks.\n",
        "\n",
        "**Big Picture**\n",
        "<img src='https://github.com/lbleal1/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/assets/multilayer_diagram_weights.png?raw=1' width=450px>\n",
        "\n",
        "***Remark:***                                             This is read from bottom to top.\n",
        "\n",
        "*Parts*\n",
        "1. **input layer**                                        \n",
        "first layer containing the inputs/\"features\" shown on the bottom\n",
        "\n",
        "2. **hidden layer**                                         \n",
        "the middle layer, that's why the notation also uses `${h_1}$ and ${h_2}\n",
        "\n",
        "3. **output layer**                                      \n",
        "the last layer \n",
        "\n",
        "\n",
        "*Computation Note*\n",
        "\n",
        "When you start stacking these individual units into layers and stacks of layers, into a network of neurons, the output of one layer of neurons becomes the input for the next layer. \n",
        "\n",
        "**Mathematically**\n",
        "\n",
        "*Hidden Layers*\n",
        "$$\n",
        "\\vec{h} = [h_1 \\, h_2] = \n",
        "\\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots \\, x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_{11} & w_{12} \\\\\n",
        "           w_{21} &w_{22} \\\\\n",
        "           \\vdots &\\vdots \\\\\n",
        "           w_{n1} &w_{n2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
        "\n",
        "$$\n",
        "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
        "$$\n",
        "\n",
        "***Remark:***\n",
        "Note that $\\mathbf{W_1}$ are the first set of weights between the input and the h's while $\\mathbf{W_2}$ are the second set of weights between the h's and the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yivqhOPWKnLS",
        "colab_type": "text"
      },
      "source": [
        "### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOWk8gAu0RXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Generate some data\n",
        "torch.manual_seed(7) # Set the random seed so things are predictable\n",
        "\n",
        "# Features are 3 random normal variables\n",
        "features = torch.randn((1, 3)) \n",
        "\n",
        "# Define the size of each layer in our network\n",
        " \n",
        "# Number of input units, must match number of input features \n",
        "# features.shape returns a list [1,3]  \n",
        "# so we can get the number of input unites with the second element thus\n",
        "n_input = features.shape[1]    \n",
        "\n",
        "n_hidden = 2                    # Number of hidden units \n",
        "n_output = 1                    # Number of output units\n",
        "\n",
        "# Weights for inputs to hidden layer\n",
        "W1 = torch.randn(n_input, n_hidden) # size = [3,2]\n",
        "# Weights for hidden layer to output layer\n",
        "W2 = torch.randn(n_hidden, n_output) # size = [2. 1]\n",
        "\n",
        "# and bias terms for hidden and output layers\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FI7xOs40RXt",
        "colab_type": "code",
        "outputId": "72370a01-2a2a-4966-8352-16bccc9a8455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "h = activation(torch.mm(features, W1) + B1)\n",
        "y =  activation(torch.mm(h,W2) + B2)\n",
        "y\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3171]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKdo0Atj0RX2",
        "colab_type": "text"
      },
      "source": [
        "***Remark:***\n",
        "\n",
        "The number of hidden units a parameter of the network, often called a **hyperparameter** to differentiate it from the weights and biases parameters. As you'll see later when we discuss training a neural network, the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNb44m7I0RX6",
        "colab_type": "text"
      },
      "source": [
        "## BONUS: Numpy to Torch and back\n",
        "\n",
        "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzZ0zUXO0RX9",
        "colab_type": "code",
        "outputId": "b6d06c50-665b-4aae-d531-4edba2837944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.rand(4,3)\n",
        "a"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54603373, 0.37691907, 0.74092096],\n",
              "       [0.06198962, 0.01454321, 0.10520602],\n",
              "       [0.85584354, 0.5030598 , 0.09519593],\n",
              "       [0.22553444, 0.78939382, 0.45312411]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aETSXGsB0RYF",
        "colab_type": "code",
        "outputId": "5d46c156-813c-4052-c204-ad717ce94294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "b = torch.from_numpy(a)\n",
        "b"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5460, 0.3769, 0.7409],\n",
              "        [0.0620, 0.0145, 0.1052],\n",
              "        [0.8558, 0.5031, 0.0952],\n",
              "        [0.2255, 0.7894, 0.4531]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yp_hvjH0RYO",
        "colab_type": "code",
        "outputId": "b5c8477c-38df-43a6-bf6d-f4dc246b6120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "b.numpy()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54603373, 0.37691907, 0.74092096],\n",
              "       [0.06198962, 0.01454321, 0.10520602],\n",
              "       [0.85584354, 0.5030598 , 0.09519593],\n",
              "       [0.22553444, 0.78939382, 0.45312411]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmWdTocZ0RYX",
        "colab_type": "text"
      },
      "source": [
        "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RU72rcM0RYb",
        "colab_type": "code",
        "outputId": "6abe1256-2b77-47cb-cf67-638c8b5e7b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Multiply PyTorch Tensor by 2, in place\n",
        "b.mul_(2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0921, 0.7538, 1.4818],\n",
              "        [0.1240, 0.0291, 0.2104],\n",
              "        [1.7117, 1.0061, 0.1904],\n",
              "        [0.4511, 1.5788, 0.9062]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4nMyLaf0RYk",
        "colab_type": "code",
        "outputId": "1336e5db-3717-4fea-b703-bf8743204f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Numpy array matches new values from Tensor\n",
        "a"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.09206746, 0.75383813, 1.48184193],\n",
              "       [0.12397924, 0.02908641, 0.21041205],\n",
              "       [1.71168707, 1.00611959, 0.19039186],\n",
              "       [0.45106888, 1.57878765, 0.90624823]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}